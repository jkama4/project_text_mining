{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a5a47f",
   "metadata": {},
   "source": [
    "# Named Entity Recognition and Classification (NERC) - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70203816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as hf_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62d65e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ner_file: str = r\"C:\\Users\\jayde\\OneDrive\\School\\Text Mining for AI\\final_project_tm\\data\\train_data\\NER-train.tsv\"\n",
    "test_data_ner_file: str = r\"C:\\Users\\jayde\\OneDrive\\School\\Text Mining for AI\\final_project_tm\\data\\test_data\\NER-test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3df14fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_data_ner_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "755db51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = utils.gather_tokens_and_tags(df=df)\n",
    "\n",
    "train_data: List[Dict[str,str]] = []\n",
    "for tokens, ner_tags in zip(X, y):\n",
    "    tokens = [str(token) for token in tokens]\n",
    "    ner_tags = [str(ner_tag) for ner_tag in ner_tags]\n",
    "    \n",
    "    train_data.append({\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": ner_tags\n",
    "    })\n",
    "    \n",
    "dataset = hf_Dataset.from_list(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "422dd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cae95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(set(label for seq in y for label in seq))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "461f20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = TOKENIZER(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[labels[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5602976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45c0860723c40a4aad4911e99d8b654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdd9da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69578503",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\jayde\\OneDrive\\School\\Text Mining for AI\\project_text_mining\\final_project_tm\\models\\bert_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ff0ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayde\\anaconda3\\envs\\text_mining_env_fixed\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"no\",\n",
    "    eval_steps=250,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.4,\n",
    "    logging_steps=100,\n",
    "    save_steps=250,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5fba874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayde\\AppData\\Local\\Temp\\ipykernel_17976\\151465286.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=TOKENIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_finetuning(trainer: Trainer):\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining_env_fixed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
