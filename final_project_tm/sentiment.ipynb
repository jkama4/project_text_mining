{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee82db11",
      "metadata": {
        "id": "ee82db11"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTxo1v-6omad"
      },
      "id": "aTxo1v-6omad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58a77de",
      "metadata": {
        "id": "a58a77de"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'train_df.csv', 'validation': 'val_df.csv', 'test': 'test_df.csv'}\n",
        "dataframes = []\n",
        "for split, filename in splits.items():\n",
        "    if filename != 'val_df.csv':\n",
        "      df = pd.read_csv(f'hf://datasets/Sp1786/multiclass-sentiment-analysis-dataset/{filename}')\n",
        "      df.drop('id', axis=1, inplace=True)\n",
        "      dataframes.append(df)\n",
        "    else:\n",
        "      test_df = pd.read_csv(f'hf://datasets/Sp1786/multiclass-sentiment-analysis-dataset/{filename}')\n",
        "      test_df.drop('id', axis=1, inplace=True)\n",
        "train_df = pd.concat(dataframes)\n",
        "train_df = train_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "print(test_df.head())\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "Zw49r1HZOksT"
      },
      "id": "Zw49r1HZOksT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9af5349",
      "metadata": {
        "id": "e9af5349"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
        "test_tokenized = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "GLda_7q4XWkK"
      },
      "id": "GLda_7q4XWkK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokenized)"
      ],
      "metadata": {
        "id": "FdrgJA8eijz9"
      },
      "id": "FdrgJA8eijz9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
        "    learning_rate=5e-5,              # Start with a small learning rate\n",
        "    per_device_train_batch_size=32,  # Batch size per GPU\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    weight_decay=0.01,               # Regularization\n",
        "    logging_steps=100,               # Log every 100 steps\n",
        "    fp16=True                        # Enable mixed precision for faster training\n",
        ")"
      ],
      "metadata": {
        "id": "w9CZVdZSdfB9"
      },
      "id": "w9CZVdZSdfB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "pVmgXjK1esZ3"
      },
      "id": "pVmgXjK1esZ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from evaluate import load\n",
        "\n",
        "# Load a metric (F1-score in this case)\n",
        "metric = load(\"f1\")\n",
        "\n",
        "# Define a custom compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels, average='weighted')"
      ],
      "metadata": {
        "id": "x6iV6aeVd3fy"
      },
      "id": "x6iV6aeVd3fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ],
      "metadata": {
        "id": "PDIGQQtaeqDY"
      },
      "id": "PDIGQQtaeqDY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                        # Pre-trained BERT model\n",
        "    args=training_args,                 # Training arguments\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,        # Efficient batching\n",
        "    compute_metrics=compute_metrics,     # Custom metric\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "SAH9GDMNe8ik"
      },
      "id": "SAH9GDMNe8ik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGZIsQdFrbfl"
      },
      "id": "pGZIsQdFrbfl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}